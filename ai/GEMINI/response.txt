```python
from manim import *
import numpy as np

class NeuralNetworkExplanation(MovingCameraScene):
    def construct(self):
        # --- Configuration and Initial Setup ---
        # Set a dark background for better contrast and a cinematic feel
        self.camera.background_color = "#1a1a2e" # A deep dark purple-blue for a techy look

        # --- 0. Title and Introduction ---
        title = Text("Understanding Neural Networks", font_size=55, color=WHITE).to_edge(UP, buff=0.8)
        subtitle = Text("From Basic Neurons to Intelligent Systems", font_size=30, color=GREY_A).next_to(title, DOWN, buff=0.3)
        
        self.play(
            FadeIn(title, shift=UP * 0.5),
            FadeIn(subtitle, shift=UP * 0.5),
            run_time=1.5
        )
        self.wait(1.5)

        # Conceptual AI brain / learning machine transition
        brain_shape = Circle(radius=1.5, color=PURPLE_C, fill_opacity=0.7).scale(0.8).move_to(ORIGIN)
        brain_label = Text("Learning Machine", font_size=28, color=WHITE).next_to(brain_shape, DOWN, buff=0.5)

        self.play(
            FadeOut(title, shift=UP * 0.5),
            FadeOut(subtitle, shift=UP * 0.5),
            FadeIn(brain_shape, scale=0.5),
            FadeIn(brain_label, shift=DOWN * 0.5)
        )
        self.wait(1)

        intro_text = Text("Inspired by the Human Brain's Learning", font_size=30, color=YELLOW_A).next_to(brain_shape, UP, buff=0.5)
        self.play(Write(intro_text))
        self.wait(1)

        # Transition to network structure by scaling down the brain
        self.play(FadeOut(intro_text), FadeOut(brain_label))
        self.play(brain_shape.animate.scale(0.5).to_corner(UL, buff=1.0))
        self.wait(0.5)

        # --- 1. The Basic Unit: The Neuron ---
        neuron_title = Text("1. The Neuron: The Fundamental Unit", font_size=40, color=YELLOW_A).to_edge(UP, buff=0.8)
        self.play(Write(neuron_title))
        self.wait(0.5)

        # Zoom in on neuron for detailed explanation
        # Save camera state to restore later
        self.camera.frame.save_state()
        self.play(self.camera.frame.animate.scale(0.8).move_to(ORIGIN), run_time=1) # Adjust zoom to fit detailed neuron
        self.wait(0.5)

        # Neuron components
        neuron_body = Circle(radius=0.7, color=BLUE_C, fill_opacity=0.8)
        neuron_label = Text("Neuron", font_size=25, color=WHITE).next_to(neuron_body, UP, buff=0.2)
        
        # Inputs (x_i)
        inputs_dots = VGroup()
        input_labels = VGroup()
        for i in range(3):
            input_dot = Dot(color=GREEN_A).next_to(neuron_body, LEFT, buff=1.5).shift(UP * (1 - i) * 0.7)
            input_val = DecimalNumber(np.random.uniform(0, 1), num_decimal_places=2, color=GREEN_A)
            input_val.next_to(input_dot, LEFT, buff=0.2)
            inputs_dots.add(input_dot)
            input_labels.add(input_val)

        input_title = Text("Inputs (x)", font_size=20, color=GREEN_A).next_to(inputs_dots, LEFT, buff=0.8).shift(LEFT * 0.5)

        # Weights (w_i) - lines connecting inputs to neuron
        weights_lines = VGroup()
        for input_dot in inputs_dots:
            weight_line = Line(input_dot.get_center(), neuron_body.get_center(), color=GREY_B, stroke_width=3)
            weights_lines.add(weight_line)
        weights_text = Text("Weights (w)", font_size=20, color=WHITE).next_to(weights_lines, DOWN, buff=0.7).shift(LEFT * 0.2)
        
        # Bias (b)
        bias_dot = Dot(color=ORANGE).next_to(neuron_body, UL, buff=0.7)
        bias_label = Text("Bias (b)", font_size=20, color=ORANGE).next_to(bias_dot, UP, buff=0.1)
        bias_line = Line(bias_dot.get_center(), neuron_body.get_center(), color=ORANGE, stroke_width=3, stroke_dash_and_gap=[0.1, 0.1])
        bias_value = DecimalNumber(np.random.uniform(-1, 1), num_decimal_places=2, color=ORANGE).next_to(bias_dot, LEFT, buff=0.2)

        # Output (y)
        output_dot = Dot(color=RED_C).next_to(neuron_body, RIGHT, buff=1.5)
        output_label = Text("Output (y)", font_size=20, color=RED_C).next_to(output_dot, RIGHT, buff=0.2)
        output_line = Line(neuron_body.get_center(), output_dot.get_center(), color=RED_C, stroke_width=3)

        self.play(
            Create(neuron_body),
            Write(neuron_label),
            FadeIn(inputs_dots),
            Write(input_title),
            FadeIn(input_labels),
            Create(output_line),
            FadeIn(output_dot),
            Write(output_label)
        )
        self.wait(0.5)

        self.play(
            Create(weights_lines), 
            Write(weights_text)
        )
        self.wait(0.5)

        self.play(
            FadeIn(bias_dot), 
            Write(bias_label), 
            Create(bias_line),
            FadeIn(bias_value)
        )
        self.wait(1)

        # Mathematical operation inside neuron
        sum_eq = MathTex(r"z = \Sigma(x_i \cdot w_i) + b", font_size=35, color=WHITE).next_to(neuron_body, RIGHT, buff=0.8).shift(LEFT * 0.7)
        activation_func_text = Text("Activation Function (f)", font_size=20, color=YELLOW_A).next_to(sum_eq, UP, buff=0.5)
        output_eq = MathTex(r"y = f(z)", font_size=35, color=WHITE).next_to(sum_eq, DOWN, buff=0.5)

        self.play(Write(sum_eq))
        self.play(Write(activation_func_text))
        self.play(Write(output_eq))
        self.wait(2)

        # Animate data flow through neuron
        value_dots = VGroup()
        for i, val in enumerate(input_labels):
            dot = Dot(color=YELLOW_A, radius=0.1).move_to(inputs_dots[i].get_center())
            value_dots.add(dot)

        bias_flow_dot = Dot(color=YELLOW_A, radius=0.1).move_to(bias_dot.get_center())

        self.play(
            FadeIn(value_dots), FadeIn(bias_flow_dot),
            run_time=0.5
        )
        
        self.play(
            LaggedStart(
                *[dot.animate.move_to(neuron_body.get_center()) for dot in value_dots],
                bias_flow_dot.animate.move_to(neuron_body.get_center()),
                lag_ratio=0.1,
                run_time=1.5
            )
        )
        self.play(FadeOut(value_dots), FadeOut(bias_flow_dot))

        output_flow_dot = Dot(color=YELLOW_A, radius=0.1).move_to(neuron_body.get_center())
        self.add(output_flow_dot)
        self.play(output_flow_dot.animate.move_to(output_dot.get_center()), run_time=1)
        self.play(FadeOut(output_flow_dot))

        # Clean up neuron details
        self.play(
            FadeOut(sum_eq),
            FadeOut(activation_func_text),
            FadeOut(output_eq),
            FadeOut(weights_text),
            FadeOut(bias_dot),
            FadeOut(bias_label),
            FadeOut(bias_line),
            FadeOut(bias_value),
            FadeOut(input_title),
            FadeOut(input_labels),
            FadeOut(inputs_dots),
            FadeOut(weights_lines),
            FadeOut(neuron_body),
            FadeOut(neuron_label),
            FadeOut(output_dot),
            FadeOut(output_label),
            FadeOut(output_line)
        )
        self.wait(0.5)
        self.play(FadeOut(neuron_title))

        # --- 2. Building Blocks: Layers ---
        self.play(Restore(self.camera.frame), run_time=1) # Restore camera view to original wide shot
        layers_title = Text("2. Layers: Input, Hidden & Output", font_size=40, color=YELLOW_A).to_edge(UP, buff=0.8)
        self.play(Write(layers_title))
        self.wait(1)

        # Create neural network structure with layers
        input_layer_nodes = VGroup(*[Dot(color=GREEN_C) for _ in range(3)]).arrange(DOWN, buff=MED_LARGE_BUFF)
        hidden_layer_1_nodes = VGroup(*[Dot(color=BLUE_C) for _ in range(4)]).arrange(DOWN, buff=MED_LARGE_BUFF)
        hidden_layer_2_nodes = VGroup(*[Dot(color=BLUE_C) for _ in range(3)]).arrange(DOWN, buff=MED_LARGE_BUFF)
        output_layer_nodes = VGroup(*[Dot(color=RED_C) for _ in range(2)]).arrange(DOWN, buff=MED_LARGE_BUFF)

        # Position layers horizontally
        input_layer_nodes.shift(LEFT * 5)
        hidden_layer_1_nodes.next_to(input_layer_nodes, RIGHT, buff=2.5)
        hidden_layer_2_nodes.next_to(hidden_layer_1_nodes, RIGHT, buff=2.5)
        output_layer_nodes.next_to(hidden_layer_2_nodes, RIGHT, buff=2.5)

        # Labels for layers
        input_label = Text("Input Layer", font_size=25, color=GREEN_A).next_to(input_layer_nodes, DOWN, buff=0.7)
        hidden_label_1 = Text("Hidden Layer 1", font_size=25, color=BLUE_A).next_to(hidden_layer_1_nodes, DOWN, buff=0.7)
        hidden_label_2 = Text("Hidden Layer 2", font_size=25, color=BLUE_A).next_to(hidden_layer_2_nodes, DOWN, buff=0.7)
        output_label = Text("Output Layer", font_size=25, color=RED_A).next_to(output_layer_nodes, DOWN, buff=0.7)

        # Create nodes and labels
        self.play(
            LaggedStart(
                FadeIn(input_layer_nodes, shift=LEFT),
                FadeIn(input_label, shift=DOWN),
                lag_ratio=0.2
            )
        )
        self.wait(0.3)
        self.play(
            LaggedStart(
                FadeIn(hidden_layer_1_nodes, shift=LEFT),
                FadeIn(hidden_label_1, shift=DOWN),
                lag_ratio=0.2
            )
        )
        self.wait(0.3)
        self.play(
            LaggedStart(
                FadeIn(hidden_layer_2_nodes, shift=LEFT),
                FadeIn(hidden_label_2, shift=DOWN),
                lag_ratio=0.2
            )
        )
        self.wait(0.3)
        self.play(
            LaggedStart(
                FadeIn(output_layer_nodes, shift=LEFT),
                FadeIn(output_label, shift=DOWN),
                lag_ratio=0.2
            )
        )
        self.wait(1)

        # Connect layers (weights/connections)
        all_connections = VGroup()
        for layer1, layer2 in zip([input_layer_nodes, hidden_layer_1_nodes, hidden_layer_2_nodes],
                                  [hidden_layer_1_nodes, hidden_layer_2_nodes, output_layer_nodes]):
            for node1 in layer1:
                for node2 in layer2:
                    line = Line(node1.get_center(), node2.get_center(), color=GREY_D, stroke_width=2)
                    all_connections.add(line)
        
        self.play(Create(all_connections, run_time=2))
        self.wait(1)

        # Group entire network for easy manipulation
        full_network = VGroup(input_layer_nodes, hidden_layer_1_nodes, hidden_layer_2_nodes, output_layer_nodes,
                              input_label, hidden_label_1, hidden_label_2, output_label,
                              all_connections)
        
        # Scale and center the network for the next sections
        self.play(FadeOut(layers_title))
        self.play(full_network.animate.scale(0.8).move_to(ORIGIN))
        self.wait(1)

        # --- 3. Functioning: The Forward Pass ---
        forward_pass_title = Text("3. Functioning: The Forward Pass", font_size=40, color=YELLOW_A).to_edge(UP, buff=0.8)
        self.play(Write(forward_pass_title))
        self.wait(1)

        # Simulate data input
        input_data_text = Text("Input Data", font_size=28, color=GREEN_A).next_to(input_layer_nodes, LEFT, buff=0.8)
        self.play(Write(input_data_text))
        self.wait(0.5)

        # Animate data flowing through the network
        flow_animations = []
        # Create initial data dots
        initial_data_dots = VGroup()
        for node in input_layer_nodes:
            initial_data_dots.add(Dot(color=GREEN_A).move_to(node.get_center() + LEFT * 0.5))
        self.play(FadeIn(initial_data_dots, shift=RIGHT*0.5))

        # Data flow through the entire network
        # This requires iterating through individual connections and animating dots
        
        # Get all paths (lines) in sequence for data flow
        all_paths_ordered = []
        for layer1, layer2 in zip([input_layer_nodes, hidden_layer_1_nodes, hidden_layer_2_nodes],
                                  [hidden_layer_1_nodes, hidden_layer_2_nodes, output_layer_nodes]):
            for node1 in layer1:
                for node2 in layer2:
                    all_paths_ordered.append(Line(node1.get_center(), node2.get_center()))

        # Animate dots moving along these paths
        # Using a slight lag for each dot to give a continuous flow appearance
        self.play(
            LaggedStart(
                *[
                    MoveAlongPath(
                        Dot(color=YELLOW_A, radius=0.08).move_to(line.get_start()), line, rate_func=linear, run_time=1.5
                    )
                    for line in all_paths_ordered
                ],
                FadeOut(initial_data_dots), # Fade out initial data dots after they "enter"
                lag_ratio=0.005 # Small lag for continuous flow
            ),
            run_time=3.0 # Overall run time for the flow
        )
        self.wait(0.5)

        prediction_text = Text("Prediction", font_size=28, color=RED_A).next_to(output_layer_nodes, RIGHT, buff=0.8)
        self.play(Write(prediction_text))
        self.wait(1.5)

        self.play(
            FadeOut(input_data_text),
            FadeOut(prediction_text),
            FadeOut(forward_pass_title)
        )
        
        # --- 4. Training: The Learning Process ---
        training_title = Text("4. Training: Learning from Errors", font_size=40, color=YELLOW_A).to_edge(UP, buff=0.8)
        self.play(Write(training_title))
        self.wait(1)

        # Show actual vs. predicted and loss calculation
        actual_value_label = Text("Actual Value", font_size=28, color=BLUE_A).to_edge(UR, buff=0.8)
        actual_val_num = DecimalNumber(0.9, num_decimal_places=2, color=BLUE_A).next_to(actual_value_label, DOWN, buff=0.2)
        
        predicted_value_label = Text("Predicted Value", font_size=28, color=RED_A).next_to(output_layer_nodes, RIGHT, buff=0.8)
        predicted_val_num = DecimalNumber(0.2, num_decimal_places=2, color=RED_A).next_to(predicted_value_label, DOWN, buff=0.2)
        
        # Shift predicted_value_label and num to be near output nodes
        predicted_value_label.move_to(output_layer_nodes.get_center() + RIGHT * 2.5 + UP * 1.5)
        predicted_val_num.next_to(predicted_value_label, DOWN, buff=0.2)

        self.play(
            FadeIn(actual_value_label), FadeIn(actual_val_num),
            FadeIn(predicted_value_label), FadeIn(predicted_val_num)
        )
        self.wait(1)

        error_arrow = CurvedArrow(predicted_val_num.get_bottom(), actual_val_num.get_top(), color=ORANGE)
        error_label = Text("Error", font_size=28, color=ORANGE).next_to(error_arrow, RIGHT, buff=0.3)
        self.play(Create(error_arrow), Write(error_label))
        self.wait(1)

        loss_function_eq = MathTex(
            r"\text{Loss} = (\text{Actual} - \text{Predicted})^2",
            font_size=35, color=WHITE
        ).to_edge(DL, buff=0.8)
        self.play(Write(loss_function_eq))
        self.wait(1.5)

        # Backpropagation Concept
        backprop_title = Text("Backpropagation: Adjusting Weights", font_size=30, color=YELLOW_A).next_to(training_title, DOWN, buff=0.3)
        self.play(Write(backprop_title))
        self.wait(0.5)

        self.play(
            FadeOut(actual_value_label), FadeOut(actual_val_num),
            FadeOut(predicted_value_label), FadeOut(predicted_val_num),
            FadeOut(error_arrow), FadeOut(error_label),
            FadeOut(loss_function_eq)
        )

        # Animate error flowing backwards and weights adjusting
        # Iterate through reversed paths (from output to input) for backprop visualization
        back_flow_animations = []
        for line in reversed(all_paths_ordered):
            dot = Dot(color=ORANGE, radius=0.08).move_to(line.get_end())
            back_flow_animations.append(MoveAlongPath(dot, line.reverse_points(), rate_func=linear, run_time=1.5))
        
        # Simulate weight adjustment by temporarily brightening connections
        self.play(
            LaggedStart(
                *back_flow_animations,
                lag_ratio=0.005 # Small lag for continuous flow
            ),
            all_connections.animate.set_color(YELLOW_A).set_stroke(width=3).set_opacity(1),
            run_time=3.0
        )
        self.wait(0.5)

        weight_adjust_text = Text("Weights are Adjusted", font_size=28, color=GREEN_A).next_to(full_network, DOWN, buff=0.7)
        self.play(Write(weight_adjust_text))
        self.play(
            all_connections.animate.set_color(GREY_B).set_stroke(width=2).set_opacity(1) # Restore color, but imply change
        )
        self.wait(1)

        self.play(
            FadeOut(weight_adjust_text),
            FadeOut(backprop_title)
        )

        # Iteration cycle and loss decrease graph
        iteration_circle = Circle(radius=0.7, color=PURPLE_A, stroke_width=5).move_to(full_network.get_center())
        iteration_text = Text("Repeat Many Times", font_size=30, color=WHITE).move_to(iteration_circle).shift(UP*0.5)
        loss_decrease_text = Text("Loss Decreases", font_size=25, color=RED_C).next_to(iteration_text, DOWN, buff=0.3)

        self.play(Create(iteration_circle))
        self.play(Write(iteration_text), Write(loss_decrease_text))
        self.wait(1)

        # Show loss graph decreasing
        loss_graph_axes = Axes(
            x_range=[0, 5, 1],
            y_range=[0, 1, 0.2],
            x_length=4,
            y_length=3,
            axis_config={"color": GREY_B, "font_size": 20},
            tips=False
        ).to_edge(DR, buff=0.8) # Position safely within the frame
        loss_graph_axes.add_coordinates()
        loss_label_x = Text("Epochs", font_size=20, color=GREY_A).next_to(loss_graph_axes.x_axis, DOWN, buff=0.1)
        loss_label_y = Text("Loss", font_size=20, color=GREY_A).next_to(loss_graph_axes.y_axis, LEFT, buff=0.1).shift(UP*0.3)

        loss_curve = ParametricFunction(
            lambda t: loss_graph_axes.c2p(t, np.exp(-t/2) * 0.8 + 0.1), # Exponential decay for loss
            x_range=[0, 5],
            color=RED_C,
            stroke_width=4
        )

        self.play(Create(loss_graph_axes), FadeIn(loss_label_x), FadeIn(loss_label_y))
        self.play(Create(loss_curve, run_time=2))
        self.wait(1.5)

        self.play(
            FadeOut(iteration_circle),
            FadeOut(iteration_text),
            FadeOut(loss_decrease_text),
            FadeOut(loss_graph_axes),
            FadeOut(loss_label_x),
            FadeOut(loss_label_y),
            FadeOut(loss_curve),
            FadeOut(training_title)
        )
        self.wait(0.5)

        # --- 5. Making Predictions (Inference) ---
        prediction_phase_title = Text("5. Making Predictions (Inference)", font_size=40, color=YELLOW_A).to_edge(UP, buff=0.8)
        self.play(Write(prediction_phase_title))
        self.wait(1)

        # Re-emphasize forward pass, now with confidence and the idea of a "trained" network
        trained_text = Text("Trained Network Ready for Use", font_size=30, color=GREEN_A).next_to(full_network, DOWN, buff=0.7)
        self.play(Write(trained_text))
        self.wait(1)

        # Simulate new input data flow
        input_data_text_2 = Text("New Input Data", font_size=28, color=GREEN_A).next_to(input_layer_nodes, LEFT, buff=0.8)
        self.play(Write(input_data_text_2))
        self.wait(0.5)

        # Initial data dots for new input
        data_dots_2 = VGroup()
        for node in input_layer_nodes:
            data_dots_2.add(Dot(color=GREEN_A).move_to(node.get_center() + LEFT * 0.5))
        self.play(FadeIn(data_dots_2, shift=RIGHT*0.5))

        # Re-run data flow animation
        self.play(
            LaggedStart(
                *[
                    MoveAlongPath(
                        Dot(color=YELLOW_A, radius=0.08).move_to(line.get_start()), line, rate_func=linear, run_time=1.5
                    )
                    for line in all_paths_ordered
                ],
                FadeOut(data_dots_2),
                lag_ratio=0.005
            ),
            run_time=3.0
        )
        self.wait(0.5)

        final_prediction_text = Text("Accurate Prediction!", font_size=30, color=RED_A).next_to(output_layer_nodes, RIGHT, buff=0.8)
        self.play(Write(final_prediction_text))
        self.wait(2)

        self.play(
            FadeOut(input_data_text_2),
            FadeOut(final_prediction_text),
            FadeOut(trained_text),
            FadeOut(prediction_phase_title)
        )
        self.wait(0.5)

        # --- 6. Conclusion ---
        conclusion_title = Text("Neural Networks in Action", font_size=45, color=WHITE).to_edge(UP, buff=0.8)
        summary_text = Text(
            "Powerful algorithms for learning from data.\n"
            "Used in image recognition, natural language processing, and more!",
            font_size=30, color=GREY_A, line_spacing=1.5
        ).next_to(conclusion_title, DOWN, buff=0.5)

        self.play(Write(conclusion_title))
        self.play(full_network.animate.center().scale(1.2), run_time=1.5) # Center and slightly enlarge the network
        self.play(Write(summary_text))
        self.wait(3)

        # Final fade out
        self.play(
            FadeOut(full_network),
            FadeOut(conclusion_title),
            FadeOut(summary_text)
        )
        self.wait(1)

```